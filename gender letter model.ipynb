{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import ipython_genutils\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, MaxPooling1D, LSTM, GRU\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import model_selection\n",
    "from sklearn import utils\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import progressbar\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lib/dataset',header=None,encoding = \"ISO-8859-1\")\n",
    "df=df.rename(columns={0: 'name'})\n",
    "tmp=df.name\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data tiap row di split karena nama dan gender masih dalam satu kolom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets = [progressbar.Percentage(),progressbar.Bar(),\" Processed : \",progressbar.Counter(),\"  \",progressbar.ETA()]\n",
    "bar = progressbar.ProgressBar(widgets=widgets, max_value=len(df.index))\n",
    "bar.start()\n",
    "\n",
    "name=[]\n",
    "gender=[]\n",
    "for index,row in bar(df.iterrows()):\n",
    "    # split\n",
    "    tmp=row['name'].split(\"@\")\n",
    "    name.append(tmp[0])\n",
    "    gender.append(tmp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buat dataframe\n",
    "df=pd.DataFrame({'name':name,'gender':gender})\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cek jumlah data tiap kelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# male   : 0\n",
    "# female : 1\n",
    "df['gender']=df['gender'].map({'m':0,'f':1})\n",
    "df.groupby('gender')['name'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "karena tidak seimbang maka data di downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataset\n",
    "df = shuffle(df)\n",
    "\n",
    "# mengambil semua data gender male & female di 2 var berbeda\n",
    "dfm=df.query('gender == 0')\n",
    "dff=df.query('gender == 1')\n",
    "\n",
    "# min jumlah terkecil dari gender male & female\n",
    "minlen=min(len(dff),len(dfm))\n",
    "\n",
    "# ambil data male & female sebanyak nilai min\n",
    "dfm=dfm.head(minlen)\n",
    "dff=dff.head(minlen)\n",
    "\n",
    "# gabung\n",
    "data_training=pd.concat([dff.name,dfm.name])\n",
    "target=pd.concat([dff.gender,dfm.gender])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nama di split menjadi list huruf, lalu cek nama yg terpanjang untuk nantinya digunakan dalam padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets = [progressbar.Percentage(),progressbar.Bar(),\" Processed : \",progressbar.Counter(),\"  \",progressbar.ETA()]\n",
    "bar = progressbar.ProgressBar(widgets=widgets, max_value=len(df.index))\n",
    "bar.start()\n",
    "\n",
    "maxleng=0\n",
    "tmp=[]\n",
    "for index,row in bar(df.iterrows()):\n",
    "    a=list(row['name'].lower())\n",
    "    tmp.append(a)\n",
    "    maxleng=max(maxleng, len(a))\n",
    "print(\"max length name : \",maxleng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " menghapus spasi yg ada di depan nama dan di akhir nama. spasi di antara nama tidak dihapus karena spasi tsb termasuk fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets = [progressbar.Percentage(),progressbar.Bar(),\" Processed : \",progressbar.Counter(),\"  \",progressbar.ETA()]\n",
    "bar = progressbar.ProgressBar(widgets=widgets, max_value=len(tmp))\n",
    "bar.start()\n",
    "\n",
    "tmp2=[]\n",
    "for letter in bar(tmp):\n",
    "    for p in range(maxleng):\n",
    "        l = len(letter)\n",
    "        if letter[0]==' ':\n",
    "            del letter[0]\n",
    "        elif letter[l-1]==' ':\n",
    "            del letter[l-1]\n",
    "    tmp2.append(letter)\n",
    "data_training=tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizing. mengkonversi list huruf menjadi vector angka. menggunakan tokenizer yg sudah dibuat sebelumnya dengan semua dataset. jika blm ada tokenizer maka dibuat terlabih dahulu dengan semua data, tidak di downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save tokenizer\n",
    "#tokenizer = Tokenizer(num_words=30)\n",
    "#tokenizer.fit_on_texts(data_training)                        \n",
    "#sequences = tokenizer.texts_to_sequences(data_training) \n",
    "#with open('tokenizer_letter.pickle', 'wb') as handle:\n",
    "#    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load tokenizer file lalu konversi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwords=30\n",
    "tokenizer = Tokenizer(num_words=nwords)\n",
    "with open('lib/tokenizer_letter.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)                                \n",
    "sequences = tokenizer.texts_to_sequences(data_training) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cek data hasil konevrsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "mlet=0\n",
    "for x in sequences:\n",
    "    for y in sequences[c]:\n",
    "        mlet=max(mlet,y)\n",
    "print(mlet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding, menyamakan length vector dengan maxlen = nama terpanjang + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = pad_sequences(sequences, maxlen=maxleng+3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split dataset menjadi data training dan data validasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_val, target_train, target_val = model_selection.train_test_split(input_seq,target,test_size = 0.2, random_state = 0)\n",
    "print('data train\\t: ',len(input_train))\n",
    "print('data validasi\\t: ',len(input_val),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cek data yg sudah di preprocess dan ready utk training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training 4 model\n",
    "1. CNN\n",
    "2. LSTM\n",
    "3. CNN LSTM\n",
    "4. CNN GRU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semua model menggunakan output fungsi aktivasi sigmoid, dg optimizer adam dan fungsi loss binary crossentropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inisiasi embed dimension, epoch dan batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "epoc=100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build model cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nwords, embed_dim,input_length = input_train.shape[1],trainable=True))\n",
    "model.add(Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set filepath utk save weights dan set callback untuk mengevaluasi bahwa hanya jika acc naik maka weights akan di save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath=\"weights_gender_letter_cnn.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "history=model.fit(input_train, target_train,validation_data=(input_val, target_val), shuffle=False, epochs=epoc, batch_size=batch_size,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights_gender_letter_cnn.hdf5')\n",
    "scores = model.evaluate(input_val, target_val)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict = model.predict_classes(input_val)\n",
    "y_true = target_val\n",
    "y_pred = predict\n",
    "target_names = ['male', 'female',]\n",
    "print (classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot acc dan loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build model lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nwords, embed_dim,input_length = input_train.shape[1],trainable=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set filepath utk save weights dan set callback untuk mengevaluasi bahwa hanya jika acc naik maka weights akan di save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath=\"weights_gender_letter_lstm.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "history=model.fit(input_train, target_train,validation_data=(input_val, target_val), shuffle=False, epochs=epoc, batch_size=batch_size,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights_gender_letter_lstm.hdf5\")\n",
    "scores = model.evaluate(input_val, target_val)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict = model.predict_classes(input_val)\n",
    "y_true = target_val\n",
    "y_pred = predict\n",
    "target_names = ['male', 'female',]\n",
    "print (classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot acc dan loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build model cnn lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nwords, embed_dim,input_length = input_train.shape[1],trainable=True))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100,return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set filepath utk save weights dan set callback untuk mengevaluasi bahwa hanya jika acc naik maka weights akan di save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath=\"weights_gender_letter_cnn-lstm.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "history=model.fit(input_train, target_train,validation_data=(input_val, target_val), shuffle=False, epochs=epoc, batch_size=batch_size,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights_gender_letter_cnn-lstm.hdf5\")\n",
    "scores = model.evaluate(input_val, target_val)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict = model.predict_classes(input_val)\n",
    "y_true = target_val\n",
    "y_pred = predict\n",
    "target_names = ['male', 'female',]\n",
    "print (classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot acc dan los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build model cnn gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nwords, embed_dim,input_length = input_train.shape[1],trainable=True))\n",
    "model.add(Conv1D(32,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(64,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.35))\n",
    "model.add(Conv1D(128,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(GRU(50,return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.45))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model_gender_letter.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set filepath utk save weights dan set callback untuk mengevaluasi bahwa hanya jika acc naik maka weights akan di save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath=\"weights_gender_letter_cnn-lstm2.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "history=model.fit(input_train, target_train,validation_data=(input_val, target_val), shuffle=False, epochs=epoc, batch_size=batch_size,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights_gender_letter_cnn-lstm2.hdf5\")\n",
    "scores = model.evaluate(input_val, target_val)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict = model.predict_classes(input_val)\n",
    "y_true = target_val\n",
    "y_pred = predict\n",
    "target_names = ['male', 'female',]\n",
    "print (classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot acc dan loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nama=\"Choiril Kurniawan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nama=nama.lower()\n",
    "nama= list(nama)\n",
    "tmp=[]\n",
    "tmp.append(nama)\n",
    "tmp.append(nama)\n",
    "sequences = tokenizer.texts_to_sequences(tmp) \n",
    "input_seq = pad_sequences(sequences, maxlen=maxleng+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# male   : 0\n",
    "# female : 1\n",
    "result=model.predict_classes(input_seq)\n",
    "print(result[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
